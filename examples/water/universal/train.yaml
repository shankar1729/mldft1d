functional:
  load-file: params.dat
  layers:
  - n-out: [10, 10]
    weight-functions: {type: gaussian, sigma_max: 2.0, degree: 1}
  - n-out: [10, 10]
    weight-functions: {type: gaussian, sigma_max: 2.0, degree: 1}
  - n-out: [20, 0]
    weight-functions: {type: gaussian, sigma_max: 2.0, degree: 1}
  readout: [100, 100, 100]
  wda: yes

data:
  filenames:
  - ../mdext/Data/random*.h5
  - ../mdext/Bulk/bulk*.h5
  train-fraction: 0.8
  weight-nc: 0.001

train:
  loss-curve: loss.dat
  save-file: params.dat
  epochs: 1000
  batch-size: 400
  method: AdamW
  lr: 1.0E-3
  weight-decay: 1.0E-2
  foreach: no  # to circumvent bug in Adam in pytorch 2.1
