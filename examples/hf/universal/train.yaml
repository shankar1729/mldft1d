include: ../train-common.yaml

functional:
  load-file: params.dat
  layers:
  - n-out: [10, 10]
    weight-functions: {type: gaussian, sigma_max: 4.0, degree: 1}
  - n-out: [10, 10]
    weight-functions: {type: gaussian, sigma_max: 4.0, degree: 1}
  - n-out: [20, 00]
    weight-functions: {type: gaussian, sigma_max: 4.0, degree: 1}
  readout: [100, 100, 100]  
  wda: yes

data:
  train-fraction: 0.8
  weight-nc: 0.01
  fuse-files: yes
  filenames: ../random_*/*.h5

train:
  epochs: 2000
  batch-size: 400
  loss_scale_E: 1.0
  loss_scale_V : 1.0
  method: Adam
  lr: 1.0E-3
  weight-decay: 1.0E-3
  foreach: no  # to circumvent bug in Adam in pytorch 2.1
