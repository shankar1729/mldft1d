functional:
  load-file: params.dat
  layers:
  - n-weights: [5, 3]
    n-outputs: 10
    hidden-sizes: [30, 30]
    activation: gelu
    weight-functions: {type: gaussian, sigma_max: 2.0, degree: 1}
  - n-weights: [5, 3]
    n-outputs: 10
    hidden-sizes: [30, 30]
    activation: gelu
    weight-functions: {type: gaussian, sigma_max: 4.0, degree: 1}
  - n-weights: [5, 3]
    n-outputs: 1
    hidden-sizes: [30, 30]
    activation: gelu
    weight-functions: {type: gaussian, sigma_max: 8.0, degree: 1}

data:
  filenames: ../../random_*/*.h5
  train-fraction: 0.8
  weight-V-by-n: yes

train:
  loss-curve: loss.dat
  save-file: params.dat
  save-interval: 20
  epochs: 1000
  batch-size: 400
  energy-loss-scale: 1.0
  method: Adam
  lr: 1.0E-3
  weight-decay: 1.0E-3
  foreach: no  # to circumvent bug in Adam in pytorch 2.1
